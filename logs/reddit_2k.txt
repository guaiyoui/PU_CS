nohup: ignoring input
/data/jianweiw/code/SSLCS/PU_CS/data_loader.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  features = torch.tensor(data_list[1], dtype=torch.float32)
/data/jianweiw/code/SSLCS/PU_CS/data_loader.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(data_list[2])
/data/jianweiw/code/SSLCS/PU_CS/utils.py:98: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/aten/src/ATen/native/Copy.cpp:250.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
tensor(indices=tensor([[   249,    969,   1408,  ..., 227156, 231579, 232594],
                       [     0,      0,      0,  ..., 232964, 232964, 232964]]),
       values=tensor([1, 1, 1,  ..., 1, 1, 1]),
       size=(232965, 232965), nnz=23213838, layout=torch.sparse_coo) tensor([[1, 9, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]) tensor(46)
cpu cpu cpu
feature process time: 261.7088s
PretrainModel(
  (Linear1): Linear(in_features=612, out_features=512, bias=True)
  (encoder): TransformerBlock(
    (att_embeddings_nope): Linear(in_features=612, out_features=512, bias=True)
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (att_dropout): Dropout(p=0.1, inplace=False)
          (output_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attention_dropout): Dropout(p=0.1, inplace=False)
        (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (layer1): Linear(in_features=512, out_features=1024, bias=True)
          (gelu): GELU(approximate='none')
          (layer2): Linear(in_features=1024, out_features=512, bias=True)
        )
        (ffn_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=512, out_features=256, bias=True)
    (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  )
  (marginloss): MarginRankingLoss()
)
link_pretrain.py:43: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
  print('total params:', sum(p.numel() for p in model.parameters()))
total params: 2863874
starting transformer to coo
start mini batch processing
adj process time: 62.1817s
starting training...
torch.Size([2000, 11, 612])
Epoch: 0001 loss_train: 344072.7188
torch.Size([2000, 11, 612])
Epoch: 0002 loss_train: 338975.0312
torch.Size([2000, 11, 612])
Epoch: 0003 loss_train: 331218.9688
torch.Size([2000, 11, 612])
Epoch: 0004 loss_train: 315113.6875
torch.Size([2000, 11, 612])
Epoch: 0005 loss_train: 296537.7188
torch.Size([2000, 11, 612])
Epoch: 0006 loss_train: 273807.6562
torch.Size([2000, 11, 612])
Epoch: 0007 loss_train: 247780.2500
torch.Size([2000, 11, 612])
Epoch: 0008 loss_train: 219594.7656
torch.Size([2000, 11, 612])
Epoch: 0009 loss_train: 190222.3125
torch.Size([2000, 11, 612])
Epoch: 0010 loss_train: 161131.6094
torch.Size([2000, 11, 612])
Epoch: 0011 loss_train: 132604.1719
torch.Size([2000, 11, 612])
Epoch: 0012 loss_train: 107315.4453
torch.Size([2000, 11, 612])
Epoch: 0013 loss_train: 86024.3750
torch.Size([2000, 11, 612])
Epoch: 0014 loss_train: 68332.0312
torch.Size([2000, 11, 612])
Epoch: 0015 loss_train: 54149.9453
torch.Size([2000, 11, 612])
Epoch: 0016 loss_train: 43609.1992
torch.Size([2000, 11, 612])
Epoch: 0017 loss_train: 35429.3164
torch.Size([2000, 11, 612])
Epoch: 0018 loss_train: 29313.3809
torch.Size([2000, 11, 612])
Epoch: 0019 loss_train: 24618.8457
torch.Size([2000, 11, 612])
Epoch: 0020 loss_train: 21106.0781
torch.Size([2000, 11, 612])
Epoch: 0021 loss_train: 18734.7910
torch.Size([2000, 11, 612])
Epoch: 0022 loss_train: 16874.9277
torch.Size([2000, 11, 612])
Epoch: 0023 loss_train: 15918.4170
torch.Size([2000, 11, 612])
Epoch: 0024 loss_train: 15282.0234
torch.Size([2000, 11, 612])
Epoch: 0025 loss_train: 14934.7559
torch.Size([2000, 11, 612])
Epoch: 0026 loss_train: 14273.9873
torch.Size([2000, 11, 612])
Epoch: 0027 loss_train: 13774.1211
torch.Size([2000, 11, 612])
Epoch: 0028 loss_train: 12787.0283
torch.Size([2000, 11, 612])
Epoch: 0029 loss_train: 12068.9316
torch.Size([2000, 11, 612])
Epoch: 0030 loss_train: 10974.1348
torch.Size([2000, 11, 612])
Epoch: 0031 loss_train: 9961.8379
torch.Size([2000, 11, 612])
Epoch: 0032 loss_train: 9107.7881
torch.Size([2000, 11, 612])
Epoch: 0033 loss_train: 8050.7842
torch.Size([2000, 11, 612])
Epoch: 0034 loss_train: 7266.6802
torch.Size([2000, 11, 612])
Epoch: 0035 loss_train: 6531.4463
torch.Size([2000, 11, 612])
Epoch: 0036 loss_train: 5812.9697
torch.Size([2000, 11, 612])
Epoch: 0037 loss_train: 5006.8638
torch.Size([2000, 11, 612])
Epoch: 0038 loss_train: 4311.5591
torch.Size([2000, 11, 612])
Epoch: 0039 loss_train: 3577.2842
torch.Size([2000, 11, 612])
Epoch: 0040 loss_train: 2998.3110
torch.Size([2000, 11, 612])
Epoch: 0041 loss_train: 2485.0200
torch.Size([2000, 11, 612])
Epoch: 0042 loss_train: 2038.5049
torch.Size([2000, 11, 612])
Epoch: 0043 loss_train: 1614.2848
torch.Size([2000, 11, 612])
Epoch: 0044 loss_train: 1415.1306
torch.Size([2000, 11, 612])
Epoch: 0045 loss_train: 1321.6101
torch.Size([2000, 11, 612])
Epoch: 0046 loss_train: 1297.0731
torch.Size([2000, 11, 612])
Epoch: 0047 loss_train: 1320.3372
torch.Size([2000, 11, 612])
Epoch: 0048 loss_train: 1332.3701
torch.Size([2000, 11, 612])
Epoch: 0049 loss_train: 1400.1215
torch.Size([2000, 11, 612])
Epoch: 0050 loss_train: 1389.3146
torch.Size([2000, 11, 612])
Epoch: 0051 loss_train: 1310.8357
torch.Size([2000, 11, 612])
Epoch: 0052 loss_train: 1225.6864
torch.Size([2000, 11, 612])
Epoch: 0053 loss_train: 1055.8022
torch.Size([2000, 11, 612])
Epoch: 0054 loss_train: 898.8421
torch.Size([2000, 11, 612])
Epoch: 0055 loss_train: 723.6160
torch.Size([2000, 11, 612])
Epoch: 0056 loss_train: 548.9279
torch.Size([2000, 11, 612])
Epoch: 0057 loss_train: 391.9418
torch.Size([2000, 11, 612])
Epoch: 0058 loss_train: 318.9223
torch.Size([2000, 11, 612])
Epoch: 0059 loss_train: 193.0721
torch.Size([2000, 11, 612])
Epoch: 0060 loss_train: 130.4453
torch.Size([2000, 11, 612])
Epoch: 0061 loss_train: 131.8784
torch.Size([2000, 11, 612])
Epoch: 0062 loss_train: 123.1596
torch.Size([2000, 11, 612])
Epoch: 0063 loss_train: 116.0160
torch.Size([2000, 11, 612])
Epoch: 0064 loss_train: 129.4160
torch.Size([2000, 11, 612])
Epoch: 0065 loss_train: 150.5503
torch.Size([2000, 11, 612])
Epoch: 0066 loss_train: 135.6557
torch.Size([2000, 11, 612])
Epoch: 0067 loss_train: 109.3315
torch.Size([2000, 11, 612])
Epoch: 0068 loss_train: 81.4567
torch.Size([2000, 11, 612])
Epoch: 0069 loss_train: 110.7341
torch.Size([2000, 11, 612])
Epoch: 0070 loss_train: 80.2348
torch.Size([2000, 11, 612])
Epoch: 0071 loss_train: 38.4950
torch.Size([2000, 11, 612])
Epoch: 0072 loss_train: 5.3409
torch.Size([2000, 11, 612])
Epoch: 0073 loss_train: -30.1948
torch.Size([2000, 11, 612])
Epoch: 0074 loss_train: -63.5395
torch.Size([2000, 11, 612])
Epoch: 0075 loss_train: -74.1950
torch.Size([2000, 11, 612])
Epoch: 0076 loss_train: -111.2552
torch.Size([2000, 11, 612])
Epoch: 0077 loss_train: -96.7118
torch.Size([2000, 11, 612])
Epoch: 0078 loss_train: -100.3728
torch.Size([2000, 11, 612])
Epoch: 0079 loss_train: -104.8546
torch.Size([2000, 11, 612])
Epoch: 0080 loss_train: -90.3396
torch.Size([2000, 11, 612])
Epoch: 0081 loss_train: -93.9557
torch.Size([2000, 11, 612])
Epoch: 0082 loss_train: -96.8874
torch.Size([2000, 11, 612])
Epoch: 0083 loss_train: -92.0381
torch.Size([2000, 11, 612])
Epoch: 0084 loss_train: -86.1701
torch.Size([2000, 11, 612])
Epoch: 0085 loss_train: -104.6614
torch.Size([2000, 11, 612])
Epoch: 0086 loss_train: -97.5194
torch.Size([2000, 11, 612])
Epoch: 0087 loss_train: -111.5304
torch.Size([2000, 11, 612])
Epoch: 0088 loss_train: -138.0498
torch.Size([2000, 11, 612])
Epoch: 0089 loss_train: -128.7589
torch.Size([2000, 11, 612])
Epoch: 0090 loss_train: -132.5258
torch.Size([2000, 11, 612])
Epoch: 0091 loss_train: -154.1507
torch.Size([2000, 11, 612])
Epoch: 0092 loss_train: -133.7049
torch.Size([2000, 11, 612])
Epoch: 0093 loss_train: -121.9413
torch.Size([2000, 11, 612])
Epoch: 0094 loss_train: -154.7789
torch.Size([2000, 11, 612])
Epoch: 0095 loss_train: -130.0614
torch.Size([2000, 11, 612])
Epoch: 0096 loss_train: -125.0832
torch.Size([2000, 11, 612])
Epoch: 0097 loss_train: -134.4438
torch.Size([2000, 11, 612])
Epoch: 0098 loss_train: -115.9166
torch.Size([2000, 11, 612])
Epoch: 0099 loss_train: -132.6969
torch.Size([2000, 11, 612])
Epoch: 0100 loss_train: -141.1599
Optimization Finished!
Train time: 6.5522s
Start Save Model...
nohup: ignoring input
/data/jianweiw/code/SSLCS/PU_CS/data_loader.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  features = torch.tensor(data_list[1], dtype=torch.float32)
/data/jianweiw/code/SSLCS/PU_CS/data_loader.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(data_list[2])
/data/jianweiw/code/SSLCS/PU_CS/utils.py:98: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/aten/src/ATen/native/Copy.cpp:250.)
  lap_pos_enc = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()
tensor(indices=tensor([[   249,    969,   1408,  ..., 227156, 231579, 232594],
                       [     0,      0,      0,  ..., 232964, 232964, 232964]]),
       values=tensor([1, 1, 1,  ..., 1, 1, 1]),
       size=(232965, 232965), nnz=23213838, layout=torch.sparse_coo) tensor([[1, 9, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]) tensor(46)
cpu cpu cpu
feature process time: 275.7435s
PretrainModel(
  (Linear1): Linear(in_features=612, out_features=512, bias=True)
  (encoder): TransformerBlock(
    (att_embeddings_nope): Linear(in_features=612, out_features=512, bias=True)
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attention_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiHeadAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (att_dropout): Dropout(p=0.1, inplace=False)
          (output_layer): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attention_dropout): Dropout(p=0.1, inplace=False)
        (ffn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (ffn): FeedForwardNetwork(
          (layer1): Linear(in_features=512, out_features=1024, bias=True)
          (gelu): GELU(approximate='none')
          (layer2): Linear(in_features=1024, out_features=512, bias=True)
        )
        (ffn_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (out_proj): Linear(in_features=512, out_features=256, bias=True)
    (attn_layer): Linear(in_features=1024, out_features=1, bias=True)
  )
  (marginloss): MarginRankingLoss()
)
link_pretrain.py:43: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
  print('total params:', sum(p.numel() for p in model.parameters()))
total params: 2863874
starting transformer to coo
start mini batch processing
start mini batch: adj of each chunks
start mini batch: minus adj of each chunks
start mini batch: back to torch coo adj
start mini batch: back to torch coo minus adj
adj process time: 60.1945s
starting training...
Epoch: 0001 loss_train: 791.2227
Epoch: 0002 loss_train: 581.9194
Epoch: 0003 loss_train: 274.7595
Epoch: 0004 loss_train: 119.8655
Epoch: 0005 loss_train: 323.9139
Epoch: 0006 loss_train: 88.1425
Epoch: 0007 loss_train: 69.0721
Epoch: 0008 loss_train: 118.6115
Epoch: 0009 loss_train: 46.2304
Epoch: 0010 loss_train: 35.9716
Epoch: 0011 loss_train: 34.5244
Epoch: 0012 loss_train: 27.5459
Epoch: 0013 loss_train: 29.8349
Optimization Finished!
Train time: 104.3463s
Start Save Model...
