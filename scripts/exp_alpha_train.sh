# nohup  python link_pretrain.py --dataset texas --model_name alpha001_texas --alpha 0.01 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset texas --model_name alpha030_texas --alpha 0.3 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset texas --model_name alpha050_texas --alpha 0.5 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset texas --model_name alpha070_texas --alpha 0.7 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset texas --model_name alpha090_texas --alpha 0.9 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cornell --model_name alpha001_cornell --alpha 0.01 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cornell --model_name alpha030_cornell --alpha 0.3 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cornell --model_name alpha050_cornell --alpha 0.5 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cornell --model_name alpha070_cornell --alpha 0.7 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cornell --model_name alpha090_cornell --alpha 0.9 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset wisconsin --model_name alpha001_wisconsin --alpha 0.01 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset wisconsin --model_name alpha030_wisconsin --alpha 0.30 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset wisconsin --model_name alpha050_wisconsin --alpha 0.50 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset wisconsin --model_name alpha070_wisconsin --alpha 0.70 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset wisconsin --model_name alpha090_wisconsin --alpha 0.90 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cora --model_name alpha001_cora --alpha 0.01 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cora --model_name alpha030_cora --alpha 0.30 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cora --model_name alpha050_cora --alpha 0.50 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cora --model_name alpha070_cora --alpha 0.70 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset cora --model_name alpha090_cora --alpha 0.90 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset citeseer --model_name alpha001_citeseer --alpha 0.01 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset citeseer --model_name alpha030_citeseer --alpha 0.30 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset citeseer --model_name alpha050_citeseer --alpha 0.50 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset citeseer --model_name alpha070_citeseer --alpha 0.70 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset citeseer --model_name alpha090_citeseer --alpha 0.90 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset photo --model_name alpha001_photo --alpha 0.01 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset photo --model_name alpha030_photo --alpha 0.30 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset photo --model_name alpha050_photo --alpha 0.50 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset photo --model_name alpha070_photo --alpha 0.70 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset photo --model_name alpha090_photo --alpha 0.90 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset dblp --model_name alpha001_dblp --alpha 0.01 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

# python link_pretrain.py --dataset dblp --model_name alpha030_dblp --alpha 0.30 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

nohup python link_pretrain.py --dataset dblp --model_name alpha050_dblp --alpha 0.50 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset dblp --model_name alpha070_dblp --alpha 0.70 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset dblp --model_name alpha090_dblp --alpha 0.90 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset cs --model_name alpha001_cs --alpha 0.01 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset cs --model_name alpha030_cs --alpha 0.30 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset cs --model_name alpha050_cs --alpha 0.50 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset cs --model_name alpha070_cs --alpha 0.70 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset cs --model_name alpha090_cs --alpha 0.90 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset physics --model_name alpha001_physics --alpha 0.01 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset physics --model_name alpha030_physics --alpha 0.30 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset physics --model_name alpha050_physics --alpha 0.50 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset physics --model_name alpha070_physics --alpha 0.70 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset physics --model_name alpha090_physics --alpha 0.90 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset reddit --model_name alpha010_reddit --alpha 0.10 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset reddit --model_name alpha030_reddit --alpha 0.30 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset reddit --model_name alpha050_reddit --alpha 0.50 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset reddit --model_name alpha070_reddit --alpha 0.70 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &&

python link_pretrain.py --dataset reddit --model_name alpha090_reddit --alpha 0.90 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_alpha_training.txt 2>&1 &

