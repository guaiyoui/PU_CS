
nohup  python exp_link_pretrain.py --dataset texas --model_name abla_contra_texas --alpha 0.00 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset texas --model_name abla_generative_texas --alpha 100.00 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset texas --model_name abla_base_texas --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset cornell --model_name abla_contra_cornell --alpha 0.00 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cornell --model_name abla_generative_cornell --alpha 100.00 --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cornell --model_name abla_base_cornell --batch_size 183 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset wisconsin --model_name abla_contra_wisconsin --alpha 0.00 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset wisconsin --model_name abla_generative_wisconsin --alpha 100.00 --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset wisconsin --model_name abla_base_wisconsin --batch_size 251 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 5 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset cora --model_name abla_contra_cora --alpha 0.00 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cora --model_name abla_generative_cora --alpha 100.0 --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cora --model_name abla_base_cora --batch_size 2708 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.01  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset citeseer --model_name abla_contra_citeseer --alpha 0.00 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset citeseer --model_name abla_generative_citeseer --alpha 100.0 --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset citeseer --model_name abla_base_citeseer --batch_size 3327  --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset photo --model_name abla_contra_photo --alpha 0.00 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset photo --model_name abla_generative_photo --alpha 100.00 --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset photo --model_name abla_base_photo --batch_size 7650 --epochs 100 --dropout 0.1 --hidden_dim 128 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset dblp --model_name abla_contra_dblp --alpha 0.00 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset dblp --model_name abla_generative_dblp --alpha 100.00 --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset dblp --model_name abla_base_dblp --batch_size 17716 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset cs --model_name abla_contra_cs --alpha 0.00 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cs --model_name abla_generative_cs --alpha 100.0 --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset cs --model_name abla_base_cs --batch_size 18333 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 3  --n_heads 8 --n_layers 3 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/cs_traiexp_abalation_trainingning.txt 2>&1 &&

python exp_link_pretrain.py --dataset physics --model_name abla_contra_physics --alpha 0.0 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset physics --model_name abla_generative_physics --alpha 100.0 --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset physics --model_name abla_base_physics --batch_size 4000 --epochs 100 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 1 --pe_dim 3 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&

python exp_link_pretrain.py --dataset reddit --model_name abla_contra_reddit --alpha 0.0 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/reddit_texp_abalation_trainingraining.txt 2>&1 &&
python exp_link_pretrain.py --dataset reddit --model_name abla_generative_reddit --alpha 100.0 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &&
python exp_link_pretrain.py --dataset reddit --model_name abla_base_reddit --alpha 0.01 --epochs 100 --batch_size 4000 --dropout 0.1 --hidden_dim 512 --hops 5  --n_heads 8 --n_layers 2 --pe_dim 10 --peak_lr 0.001  --weight_decay=1e-05 --device 1 >> ./logs/exp_abalation_training.txt 2>&1 &
